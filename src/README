This directory contains the sources aiming at measuring lightcurves of
point-like varying objects, via a simultaneous fit to a set of
images. The concept is the one proposed in Sebatien Fabbro's PhD
thesis. The main variant is that here, the images are not assumed to
be "aligned" together (i.e. on the same pixel grid).It aims at
implementing the same functionalities as the lc directory, without
resampling input images.


classes:

LightCurveFit :
reads the so-called lightcurvefile. At the moment, in the same
format as before.

SimPhotFit:
actually does the fit of a single object.
it inherits of Model that holds a few things which define the model
being fitted.

Vignette:
there is one per image involved in the fit. Handles the data 
assciated to this image : pixels, weights geom transfos to the geometric 
reference, convolution kernel.


There is some provision for exception handling, but many
problems are handled in the traditional way.


auxiliary classes:

IntFrame : a poloka Frame, with integer coordinates : xmin, xmax, ymin, ymax.
The convention is that xmin is inside, and xmax is not ([xmin,xmax[)


template Array2D<T> : public IntFrame
contains pixels of type T indexed by coordinates inside the IntFrame.

CoeffBlock  = Array2D<float>

Array4D : public Array2D<CoeffBlock>
This is a "4-index tensor", used to hold the derivatives of an image
pixels w.r.t to model pixels. It saves a lot of memory compared to a 
flat 4 dimension array, because these derivatives are sparse.

PixelBlock : public Array2D<double>
a handle to hold pixels, weights, kernels. Has FITS IOs, and
basic algebra. The difference with DImage and Kernel, is that
the limits are arbitrary.

All these 2D and 4D objects have matrix-like "accessors" : A(i,j).
These accessors involve a bound checking if desired at compile time.
Going beyond the fence results in a core dump.  Any change to the code
has to be tested with bound checking before running without bound
checking: it is preferable to waste CPU checking bounds than debugging
a production messed up by fence violations...


Notes about the used strategy.
-----------------------------

  We need: 
- PSF's
-convolution kernels
- geometrical transformations.

In the approach where convolution kernels are fitted on resampled images,
a small transformation innacuracy is absorbed when fitting the convolution kernel.

Here, the idea is to derive convolution kernels from the PSF's. There
can be systematic position offsets involved, which have to be traced.
As a rule, keep in mind that if you fit a star with a fixed position,
the relative flux bias is 0.5*(delta/sigma)**2, where delta is the
difference between assumed and true position, and sigma is the star r.m.s
(the r.m.s IQ).

  To get the PSF, we ask for a PSF at a given position (provided with
  sub-pixel precision). The centroid of the obtained PSF is not
  necessarily the user provided position. I saw offsets of 0.1 pixel.
  We may consider setting that to 0, during the PSF construction.
  This means adding constraints in the fit performed in
  ImagePSF::StackResiduals, since the used analytic profile are
  symetric (with however some errors introduced by the coarse
  integration of the profile over the pixel).  However, it is not
  clear that we want to set that to zero: the difference between the
  provided PSF position and the centroid is due to PSF asymetries.  Do
  we want to define object positions as the centroid of an object or
  as the central position of a fitted symetric profile? The later has
the great advantage of being much less noisier than the former, and 
on actually asymetric profiles, it is independent of the S/N, whereas the
centroid computed using pixels over a given cut does depend on the cut.

  The second question is how we relate image positions. here again, we may 
use centroids or profile fitted  positions. The latter are less noisy
by about a factor of 2. Although we don't really care about alignment
residuals being 0.02 or 0.01 pixel, why should we not use the best position
estimator we have?

  So, I guess that the right way to go is to align images using profile fitted
coordinates,



Pierre Astier (02/06)






